{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Foreset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier, RandomForestRegressor\n",
    "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit, train_test_split, GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler, RobustScaler\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = []\n",
    "with open('kddcup.names') as f:\n",
    "    next(f)\n",
    "    for line in f:\n",
    "        feature = line.split(':')[0]\n",
    "        columns.append(feature)\n",
    "columns.append('label')\n",
    "\n",
    "train_data = pd.read_csv('kddcup.data', header=None)\n",
    "train_data.columns = columns\n",
    "train_data['label'] = train_data['label'].apply(lambda x: x.replace('.', ''))\n",
    "\n",
    "test_data = pd.read_csv('corrected', header=None)\n",
    "test_data.columns = columns\n",
    "test_data['label'] = test_data['label'].apply(lambda x: x.replace('.', ''))\n",
    "\n",
    "X_data = train_data.drop(columns=['label'])\n",
    "y_data = train_data['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split categorical attributes and continuous attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_type = []\n",
    "with open('kddcup.names') as f:\n",
    "    next(f)\n",
    "    for line in f:\n",
    "        feature = line.split(': ')[1]\n",
    "        feature_type.append(feature[0:-2])\n",
    "feature_type.append('label')\n",
    "\n",
    "# Split features by type\n",
    "data_index = X_data.index.values\n",
    "idx_symbolic = [index for index, ft_type in enumerate(feature_type) if ft_type == \"symbolic\"]\n",
    "idx_continuous = [index for index, ft_type in enumerate(feature_type) if ft_type == \"continuous\"]\n",
    "X_symbolic = X_data.iloc[:, idx_symbolic]\n",
    "X_continuous = X_data.iloc[:, idx_continuous]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Outlier by continuous attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove outlier\n",
    "mean = X_continuous.describe().loc['mean']\n",
    "std = X_continuous.describe().loc['std']\n",
    "\n",
    "num_attributes = len(X_continuous.columns)\n",
    "instances_as_outliers = []\n",
    "\n",
    "for attribute in X_continuous:\n",
    "    attr = X_continuous[attribute]\n",
    "    upper_bound = mean[attribute]+3*std[attribute]\n",
    "    instances_as_outliers.append(X_continuous[attr>upper_bound].index)\n",
    "\n",
    "filtered_idx = []\n",
    "for indices in instances_as_outliers:\n",
    "    for idx in indices:\n",
    "        filtered_idx.append(idx)\n",
    "\n",
    "filtered_idx = set(filtered_idx)\n",
    "\n",
    "X_symbolic = X_symbolic.drop(index = filtered_idx, axis=0)\n",
    "X_continuous = X_continuous.drop(index = filtered_idx, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature standardisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardise continuous features.\n",
    "sscaler = StandardScaler().fit(X_continuous)\n",
    "X_continuous = sscaler.transform(X_continuous)\n",
    "\n",
    "# Encode labels\n",
    "y_data_clean = y_data.drop(index = filtered_idx, axis=0)\n",
    "encoder = LabelBinarizer()\n",
    "y_data_clean = encoder.fit_transform(y_data_clean)\n",
    "\n",
    "# Combine two types of features \n",
    "#X_data_clean = pd.concat([X_symbolic, X_continuous], axis=1)\n",
    "#X_data_clean.describe()\n",
    "\n",
    "# Split training data into trainig set and validation set\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_continuous, y_data_clean, test_size=0.1, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = [int(x) for x in np.linspace(200, 2000, num = 10)]\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "\n",
    "\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'criterion': ['mse', 'mae'],\n",
    "               'max_features': ['auto', 'sqrt'],\n",
    "               'max_depth': max_depth,\n",
    "               'bootstrap': [True, False]}\n",
    "\n",
    "rf = RandomForestRegressor()\n",
    "rf_random = RandomizedSearchCV(estimator = rf, \n",
    "                               param_distributions = random_grid, \n",
    "                               n_iter = 100, \n",
    "                               verbose=2, \n",
    "                               cv = None,\n",
    "                               random_state=42, \n",
    "                               n_jobs = -1)\n",
    "\n",
    "rf_random.fit(X_train, y_train)\n",
    "\n",
    "#Random Search result \n",
    "np.save('best_rf_random.npy',rf_random.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\dme\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9999181046365658"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestRegressor()\n",
    "rf.fit(X_train, y_train)\n",
    "rf.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
